"""
üß† RAG ENGINE - –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å Re-ranking –¥–ª—è Shukabase

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Google Gemini API
2. FAISS –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
3. Re-ranking —Å –ø–æ–º–æ—â—å—é Jina Reranker
4. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
5. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Vector + BM25 + Simple Keyword)
"""

import json
import numpy as np
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Any
import logging
import os
import time
import re
import difflib

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
try:
    import faiss
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    import google.generativeai as genai
    from dotenv import load_dotenv
    from rank_bm25 import BM25Okapi
    from nltk.stem import SnowballStemmer
except ImportError as e:
    raise ImportError(
        f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: {e}. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã: pip install faiss-cpu transformers torch google-generativeai python-dotenv rank_bm25 nltk"
    )

logger = logging.getLogger(__name__)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (QueryExpander, RerankerModel) –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---

class QueryExpander:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    SYNONYMS_RU = {
        "–ª—é–±–æ–≤—å": ["–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å", "–±—Ö–∞–∫—Ç–∏", "–¥—Ä—É–∂–±–∞", "–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å", "prema"],
        "–±–æ–≥": ["–∫—Ä–∏—à–Ω–∞", "–≤–µ—Ä—Ö–æ–≤–Ω—ã–π", "–∞–±—Å–æ–ª—é—Ç", "–±–æ–∂–µ—Å—Ç–≤–æ", "–≤–∏—à–Ω—É", "–Ω–∞—Ä–∞—è–Ω–∞", "–≥–æ—Å–ø–æ–¥—å"],
        "–¥—É—à–∞": ["–∞—Ç–º–∞", "–¥—É—Ö", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Å—É—â–Ω–æ—Å—Ç—å", "–¥–∂–∏–≤–∞"],
        "–∑–Ω–∞–Ω–∏–µ": ["–¥–∂–Ω—è–Ω–∞", "–º—É–¥—Ä–æ—Å—Ç—å", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–≤–µ–¥–∞"],
        "–π–æ–≥–∞": ["–ø—Ä–∞–∫—Ç–∏–∫–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–ø—É—Ç—å", "—Å–∞–¥—Ö–∞–Ω–∞"],
        "–∫–∞—Ä–º–∞": ["–¥–µ–π—Å—Ç–≤–∏–µ", "–¥–µ—è–Ω–∏–µ", "—Å–ª–µ–¥—Å—Ç–≤–∏–µ", "—Å—É–¥—å–±–∞", "–∫–∞—Ä–º–∏—á–µ—Å–∫–∏–π"],
        "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ": ["–º–æ–∫—à–∞", "—Å–ø–∞—Å–µ–Ω–∏–µ", "—Å–≤–æ–±–æ–¥–∞", "–≤—ã—Ö–æ–¥", "–Ω–∏—Ä–≤–∞–Ω–∞"],
        "–º–∏—Ä": ["–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π", "–≤—Å–µ–ª–µ–Ω–Ω–∞—è", "–≤—Ä–µ–º–µ–Ω–Ω—ã–π", "–ø—Ä–µ—Ö–æ–¥—è—â–∏–π", "–º–∞–π—è", "–∏–ª–ª—é–∑–∏—è"],
        "–≥—É–Ω–∞": ["–∫–∞—á–µ—Å—Ç–≤–æ", "—Å–≤–æ–π—Å—Ç–≤–æ", "–ø—Ä–∏—Ä–æ–¥–∞", "—Å–∞—Ç—Ç–≤–∞", "—Ä–∞–¥–∂–∞—Å", "—Ç–∞–º–∞—Å"],
        "–ø—Ä–µ–¥–∞–Ω–Ω—ã–π": ["–≤–∞–π—à–Ω–∞–≤", "–±—Ö–∞–∫—Ç–∞", "—Å–ª—É–≥–∞", "—Å–∞–¥—Ö—É"],
        "—É—á–∏—Ç–µ–ª—å": ["–≥—É—Ä—É", "–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫", "–∞—á–∞—Ä—å—è", "—Å–≤–∞–º–∏", "–ø—Ä–∞–±—Ö—É–ø–∞–¥–∞"]
    }
    
    SYNONYMS_EN = {
        "love": ["devotion", "bhakti", "affection", "attachment", "prema"],
        "god": ["krishna", "supreme", "absolute", "deity", "vishnu", "narayana", "lord"],
        "soul": ["atma", "spirit", "consciousness", "essence", "jiva"],
        "knowledge": ["jnana", "wisdom", "understanding", "realization", "veda"],
        "yoga": ["practice", "meditation", "discipline", "path", "sadhana"],
        "karma": ["action", "deed", "consequence", "fate"],
        "liberation": ["moksha", "salvation", "freedom", "release", "nirvana"],
        "world": ["material", "universe", "temporary", "transient", "maya", "illusion"],
        "mode": ["guna", "quality", "nature", "sattva", "rajas", "tamas"],
        "devotee": ["vaishnava", "bhakta", "servant", "sadhu"],
        "teacher": ["guru", "master", "acharya", "swami", "prabhupada"]
    }
    
    @staticmethod
    def _fuzzy_find(term: str, collection: List[str], cutoff: float = 0.8) -> List[str]:
        return difflib.get_close_matches(term, collection, n=1, cutoff=cutoff)

    @staticmethod
    def expand_query_ru(query: str) -> List[str]:
        expanded = {query}
        query_words = query.lower().split()
        
        for word in query_words:
            # 1. Check keys
            for key, synonyms in QueryExpander.SYNONYMS_RU.items():
                if key == word or QueryExpander._fuzzy_find(word, [key]):
                    expanded.add(key)
                    expanded.update(synonyms)
                
                # 2. Check values (synonyms)
                if word in synonyms or QueryExpander._fuzzy_find(word, synonyms):
                    expanded.add(key)
                    expanded.update(synonyms)
                    
        return list(expanded)[:5]
    
    @staticmethod
    def expand_query_en(query: str) -> List[str]:
        expanded = {query}
        query_words = query.lower().split()
        
        for word in query_words:
            # 1. Check keys
            for key, synonyms in QueryExpander.SYNONYMS_EN.items():
                if key == word or QueryExpander._fuzzy_find(word, [key]):
                    expanded.add(key)
                    expanded.update(synonyms)
                
                # 2. Check values
                if word in synonyms or QueryExpander._fuzzy_find(word, synonyms):
                    expanded.add(key)
                    expanded.update(synonyms)
                    
        return list(expanded)[:5]


class RerankerModel:
    """–ú–æ–¥–µ–ª—å re-ranking –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, model_name: str = "jinaai/jina-reranker-v2-base-multilingual"):
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å re-ranking: {model_name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name, trust_remote_code=True, torch_dtype=torch.float32
            )
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å re-ranking –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (device: {self.device})")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ re-ranking: {e}")
            self.model = None
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float, str]]:
        if not self.model or not documents:
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    [[query, doc] for doc in documents],
                    padding=True, truncation=True, return_tensors="pt", max_length=512
                ).to(self.device)
                scores = self.model(**inputs, return_dict=True).logits.squeeze(-1).cpu().numpy()
            
            ranked = sorted([(i, score, documents[i]) for i, score in enumerate(scores)], key=lambda x: x[1], reverse=True)
            return ranked[:top_k]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ re-ranking: {e}")
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π RAGEngine ---

class RAGEngine:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å RAG —Å–∏—Å—Ç–µ–º—ã —Å Google Gemini API"""
    
    def __init__(
        self,
        reranker_model: str = "jinaai/jina-reranker-v2-base-multilingual",
        languages: List[str] = ['ru', 'en'],
        base_dir: str = "rag"
    ):
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é RAG Engine...")
        
        self._configure_gemini_api()
        
        self.base_dir = Path(base_dir)
        self.embedding_model_name = "models/text-embedding-004"
        self.languages = languages
        
        self.reranker = RerankerModel(reranker_model)
        
        self.stemmers = {
            'ru': SnowballStemmer('russian'),
            'en': SnowballStemmer('english')
        }
        
        self.indices: Dict[str, faiss.Index] = {}
        self.bm25_indices: Dict[str, Any] = {}
        self.metadata: Dict[str, Any] = {}
        self.chunked_data: Dict[str, Dict] = {}
        
        for lang in languages:
            self._load_language_data(lang)
        
        logger.info("‚úÖ RAG Engine –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _configure_gemini_api(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–ª—é—á API –¥–ª—è Gemini."""
        load_dotenv()
        api_key = os.environ.get('GEMINI_API_KEY')
        self.current_api_key = None
        
        if not api_key:
            logger.warning("‚ö†Ô∏è –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. RAG –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.")
            return

        try:
            genai.configure(api_key=api_key)
            self.current_api_key = api_key
            logger.info("‚úÖ –ö–ª—é—á Gemini API —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω.")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini API: {e}")

    def _load_language_data(self, language: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —á–∞–Ω–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."""
        index_file = self.base_dir / f"faiss_index_{language}.bin"
        metadata_file = self.base_dir / f"faiss_metadata_{language}.json"
        chunks_file = self.base_dir / f"chunked_scriptures_{language}.json"

        if not index_file.exists():
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å FAISS –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            return
            
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è —è–∑—ã–∫–∞ '{language}'...")
        self.indices[language] = faiss.read_index(str(index_file))
        logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.indices[language].ntotal:,} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ {index_file}")

        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                raw_metadata = json.load(f)
            
            # Flatten metadata to match FAISS indices
            flat_metadata = []
            structure = raw_metadata.get('structure', {})
            
            # Collect all chapters
            all_chapters = []
            for book_key, book_data in structure.items():
                for chapter_key, chapter_data in book_data.items():
                    if 'embedding_key' in chapter_data:
                        all_chapters.append({
                            'book': book_key,
                            'chapter': chapter_key,
                            'data': chapter_data
                        })
            
            # Sort by embedding_key index (e.g., embeddings_0, embeddings_1)
            def get_embedding_index(item):
                key = item['data']['embedding_key']
                try:
                    return int(key.split('_')[1])
                except (IndexError, ValueError):
                    return 999999
            
            all_chapters.sort(key=get_embedding_index)
            
            # Create flat list
            for item in all_chapters:
                book = item['book']
                chapter = item['chapter']
                data = item['data']
                num_chunks = data.get('num_chunks', 0)
                text_previews = data.get('text_previews', [])
                
                for i in range(num_chunks):
                    preview = text_previews[i] if i < len(text_previews) else ""
                    flat_metadata.append({
                        'book': book,
                        'chapter': chapter,
                        'chunk_idx': i,
                        'text_preview': preview,
                        'html_path': data.get('html_path')
                    })
            
            self.metadata[language] = flat_metadata
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ({len(flat_metadata)} –∑–∞–ø–∏—Å–µ–π)")
        else:
            logger.warning(f"  - –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_file}")

        if chunks_file.exists():
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunked_data[language] = json.load(f)
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã —á–∞–Ω–∫–∏ –∏–∑ {chunks_file}")
        else:
             logger.warning(f"  - –§–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {chunks_file}")

        # --- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–ª–∏ –ó–∞–≥—Ä—É–∑–∫–∞ BM25 –∏–Ω–¥–µ–∫—Å–∞ ---
        bm25_file = self.base_dir / f"bm25_index_{language}.pkl"

        if language in self.metadata and self.metadata[language]:
            if bm25_file.exists():
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è —è–∑—ã–∫–∞ '{language}' –∏–∑ —Ñ–∞–π–ª–∞...")
                try:
                    with open(bm25_file, 'rb') as f:
                        self.bm25_indices[language] = pickle.load(f)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ BM25 –∏–Ω–¥–µ–∫—Å–∞: {e}. –ë—É–¥—É —Å—Ç—Ä–æ–∏—Ç—å –∑–∞–Ω–æ–≤–æ.")

            if language not in self.bm25_indices:
                logger.info(f"‚è≥ –°—Ç—Ä–æ—é –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è —è–∑—ã–∫–∞ '{language}'...")
                try:
                    corpus = []
                    for meta in self.metadata[language]:
                        text = self._get_text_from_meta(meta, language)
                        corpus.append(self._tokenize(text, language))
                    
                    self.bm25_indices[language] = BM25Okapi(corpus)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 –ø–æ—Å—Ç—Ä–æ–µ–Ω ({len(corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    
                    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –∏–Ω–¥–µ–∫—Å BM25 –≤ —Ñ–∞–π–ª {bm25_file}...")
                    with open(bm25_file, 'wb') as f:
                        pickle.dump(self.bm25_indices[language], f)
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å BM25 —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ BM25: {e}")

    def _get_embedding(self, texts: List[str], api_key: str = None) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Gemini API."""
        if api_key and api_key != self.current_api_key:
            try:
                masked_key = f"{api_key[:4]}...{api_key[-4:]}" if len(api_key) > 8 else "***"
                logger.info(f"üîë Using dynamic API key: {masked_key}")
                genai.configure(api_key=api_key)
                self.current_api_key = api_key
            except Exception as e:
                logger.error(f"Error configuring API key: {e}")

        try:
            if len(texts) == 1:
                result = genai.embed_content(
                    model="models/text-embedding-004",
                    content=texts[0],
                    task_type="RETRIEVAL_QUERY"
                )
                embedding = result['embedding']
                return np.array([embedding], dtype='float32')
            else:
                all_embeddings = []
                for text in texts:
                    result = genai.embed_content(
                        model="models/text-embedding-004",
                        content=text,
                        task_type="RETRIEVAL_QUERY"
                    )
                    all_embeddings.append(result['embedding'])
                return np.array(all_embeddings, dtype='float32')
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ—Ç Gemini API: {e}", exc_info=True)
            dim = 768
            return np.zeros((len(texts), dim), dtype='float32')

    def _tokenize(self, text: str, language: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ —Å—Ç–µ–º–º–∏–Ω–≥–æ–º –¥–ª—è BM25"""
        words = re.findall(r'\w+', text.lower())
        stemmer = self.stemmers.get(language)
        if stemmer:
            return [stemmer.stem(w) for w in words]
        return words

    def _get_text_from_meta(self, meta: Dict, language: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"""
        book = meta.get('book')
        chapter = meta.get('chapter')
        chunk_idx = meta.get('chunk_idx')
        
        text = ""
        chunks_map = self.chunked_data.get(language, {})
        
        if book and chapter and book in chunks_map and chapter in chunks_map[book]:
            chapter_chunks = chunks_map[book][chapter]
            if isinstance(chapter_chunks, list) and isinstance(chunk_idx, int):
                if 0 <= chunk_idx < len(chapter_chunks):
                    text = chapter_chunks[chunk_idx]
        
        if not text:
            text = meta.get('text_preview', '')
            
        return text

    def _search_by_keyword(self, query: str, language: str, top_k: int) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –ø–æ–º–æ—â—å—é BM25"""
        bm25 = self.bm25_indices.get(language)
        if not bm25: return []
        
        try:
            tokenized_query = self._tokenize(query, language)
            scores = bm25.get_scores(tokenized_query)
            
            top_n_indices = np.argsort(scores)[::-1][:top_k]
            
            results = []
            metadata_list = self.metadata.get(language, [])
            
            for idx in top_n_indices:
                score = scores[idx]
                if score <= 0: continue
                
                meta = metadata_list[idx] if idx < len(metadata_list) else {}
                text = self._get_text_from_meta(meta, language)
                
                results.append({
                    'index': int(idx),
                    'distance': 0.0,
                    'score': float(score),
                    'text': text,
                    'book': meta.get('book'), 
                    'chapter': meta.get('chapter'), 
                    'verse': None, 
                    'chunk_idx': meta.get('chunk_idx'),
                    'html_path': meta.get('html_path'),
                    'source': 'bm25'
                })
            
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ keyword –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def _search_by_simple_match(self, query: str, language: str, top_k: int) -> List[Dict[str, Any]]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –ø–æ–¥—Å—Ç—Ä–æ–∫–∏.
        –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å 'index', —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å RRF —Å–ª–∏—è–Ω–∏–µ–º.
        """
        metadata_list = self.metadata.get(language, [])
        if not metadata_list:
            return []

        search_query = query.lower().strip()
        results = []

        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å
        for idx, meta in enumerate(metadata_list):
            text = self._get_text_from_meta(meta, language)
            lower_text = text.lower()
            
            if search_query in lower_text:
                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
                count = lower_text.count(search_query)
                
                results.append({
                    'index': int(idx),
                    'distance': 0.0,
                    'score': float(count), # Score = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π
                    'text': text,
                    'book': meta.get('book'),
                    'chapter': meta.get('chapter'),
                    'verse': None,
                    'chunk_idx': meta.get('chunk_idx'),
                    'html_path': meta.get('html_path'),
                    'source': 'simple_match'
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–∂–¥–µ–Ω–∏–π
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]

    def _search_by_vector(self, query_embedding: np.ndarray, language: str, top_k: int, vector_distance_threshold: float = None) -> List[Dict[str, Any]]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ FAISS."""
        index = self.indices.get(language)
        if not index: return []

        try:
            query_norm = query_embedding.copy().reshape(1, -1)
            faiss.normalize_L2(query_norm)
            distances, indices_found = index.search(query_norm, top_k * 2)
            
            results = []
            metadata_list = self.metadata.get(language, [])
            
            seen_ids = set()
            
            for i, (dist, idx) in enumerate(zip(distances[0], indices_found[0])):
                if idx < 0: continue

                if vector_distance_threshold is not None and dist > vector_distance_threshold:
                    continue

                meta = metadata_list[idx] if isinstance(metadata_list, list) and idx < len(metadata_list) else {}
                book, chapter = meta.get('book'), meta.get('chapter')
                chunk_idx = meta.get('chunk_idx')
                
                unique_id = f"{book}_{chapter}_{chunk_idx}"
                if unique_id in seen_ids:
                    continue
                seen_ids.add(unique_id)
                
                text = self._get_text_from_meta(meta, language)
                if not text:
                    text = meta.get('text_preview', '') + '...'

                results.append({
                    'index': int(idx),
                    'distance': float(dist),
                    'score': float(1.0 / (1.0 + dist)),
                    'text': text,
                    'book': book, 
                    'chapter': chapter, 
                    'verse': None, 
                    'chunk_idx': chunk_idx,
                    'html_path': meta.get('html_path'),
                    'source': 'vector'
                })
                
                if len(results) >= top_k:
                    break

            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –≤–µ–∫—Ç–æ—Ä—É ({language}): {e}", exc_info=True)
            return []

    def _detect_verse_reference(self, query: str) -> Dict[str, Any]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å—Ç–∏—Ö."""
        query = query.lower().strip()
        
        book_map = {
            'bg': 'bg', '–±–≥': 'bg', 'gita': 'bg', '–≥–∏—Ç–∞': 'bg', 'bhagavad': 'bg', 'bhagavad gita': 'bg', '–±—Ö–∞–≥–∞–≤–∞–¥ –≥–∏—Ç–∞': 'bg',
            'sb': 'sb', '—à–±': 'sb', 'bhagavatam': 'sb', '–±—Ö–∞–≥–∞–≤–∞—Ç–∞–º': 'sb', 'srimad bhagavatam': 'sb', '—à—Ä–∏–º–∞–¥ –±—Ö–∞–≥–∞–≤–∞—Ç–∞–º': 'sb',
            'cc': 'cc', '—á—á': 'cc', 'caitanya': 'cc', '—á–∞–π—Ç–∞–Ω—å—è': 'cc', 'caitanya caritamrta': 'cc', '—á–∞–π—Ç–∞–Ω—å—è —á–∞—Ä–∏—Ç–∞–º—Ä–∏—Ç–∞': 'cc',
            'iso': 'iso', '–∏—à–æ': 'iso', 'isopanisad': 'iso', 'sri isopanisad': 'iso', '—à—Ä–∏ –∏—à–æ–ø–∞–Ω–∏—à–∞–¥': 'iso',
            'nod': 'nod', '–Ω–ø': 'nod', 'nectar of devotion': 'nod',
            'noi': 'noi', '–Ω–Ω': 'noi', 'nectar of instruction': 'noi'
        }
        
        match = re.search(r'([a-z–∞-—è\s]+?)\.?\s*(\d+)[. :](\d+)', query)
        if match:
            book_raw, chapter, verse = match.groups()
            book_key = book_raw.strip()
            if book_key in book_map:
                return {'book': book_map[book_key], 'chapter': chapter, 'verse': verse}
        
        match_sb = re.search(r'([a-z–∞-—è\s]+?)\.?\s*(\d+)\.(\d+)\.(\d+)', query)
        if match_sb:
            book_raw, canto, chapter, verse = match_sb.groups()
            book_key = book_raw.strip()
            if book_key in book_map:
                return {'book': book_map[book_key], 'chapter': f"{canto}.{chapter}", 'verse': verse}

        return None

    def _find_verse_in_metadata(self, ref: Dict[str, Any], language: str) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å—Ç–∏—Ö –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        results = []
        metadata_list = self.metadata.get(language, [])
        
        target_book = ref['book']
        target_chapter = ref['chapter']
        target_verse = ref['verse']
        
        logger.info(f"üéØ –ò—â—É —Å—Ç–∏—Ö: Book={target_book}, Chapter={target_chapter}, Verse={target_verse}")
        
        for idx, meta in enumerate(metadata_list):
            if meta.get('book') == target_book:
                meta_chapter = str(meta.get('chapter', ''))
                
                def normalize_chapter(ch):
                    return '.'.join([p.lstrip('0') for p in str(ch).split('.')])
                
                if normalize_chapter(meta_chapter) == normalize_chapter(target_chapter):
                    
                    text = self._get_text_from_meta(meta, language)
                    clean_text = text.lower()
                    
                    is_match = False
                    
                    if f"text {target_verse}" in clean_text[:50]:
                        is_match = True
                    elif f"—Ç–µ–∫—Å—Ç {target_verse}" in clean_text[:50]:
                        is_match = True
                    elif clean_text.strip().startswith(f"{target_verse}."):
                        is_match = True
                    elif f"{target_verse}-" in clean_text[:20]:
                        is_match = True
                        
                    if is_match:
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π —Å—Ç–∏—Ö –≤ –∏–Ω–¥–µ–∫—Å–µ {idx}")
                        results.append({
                            'index': int(idx),
                            'distance': 0.0,
                            'score': 100.0,
                            'text': text,
                            'book': target_book, 
                            'chapter': meta_chapter, 
                            'verse': target_verse, 
                            'chunk_idx': meta.get('chunk_idx'),
                            'html_path': meta.get('html_path'),
                            'source': 'exact_verse'
                        })
        
        return results

    def search(
        self, 
        query: str, 
        language: str = 'ru', 
        top_k: int = 5, 
        use_reranking: bool = True,
        expand_query: bool = True,
        vector_distance_threshold: float = None,
        api_key: str = None
    ) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞.
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç: Exact Verse + Vector Search + BM25 + Simple Keyword Search
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' ({language}, top_k={top_k})")
        if language not in self.indices:
            return {'success': False, 'error': f'–ò–Ω–¥–µ–∫—Å –¥–ª—è —è–∑—ã–∫–∞ {language} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.'}

        try:
            # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—á–Ω—ã–π —Å—Ç–∏—Ö
            verse_ref = self._detect_verse_reference(query)
            exact_results = []
            if verse_ref:
                exact_results = self._find_verse_in_metadata(verse_ref, language)
                if exact_results:
                    logger.info(f"üéâ –ù–∞–π–¥–µ–Ω—ã —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç–∏—Ö–æ–≤: {len(exact_results)}")
                    return {
                        'success': True,
                        'results': exact_results,
                        'query': query,
                        'search_type': 'exact_verse_reference',
                        'count': len(exact_results)
                    }

            # 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            query_variants = [query]
            if expand_query:
                expander_method = getattr(QueryExpander, f'expand_query_{language}', None)
                if expander_method:
                    query_variants = expander_method(query)

            logger.info(f"   üìã –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞: {query_variants}")

            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            variant_embeddings = self._get_embedding(query_variants, api_key=api_key)
            
            # 3. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            all_vector_results = []
            for idx, emb in enumerate(variant_embeddings):
                vector_results = self._search_by_vector(emb, language, top_k * 2, vector_distance_threshold)
                all_vector_results.extend(vector_results)

            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            seen_indices = set()
            unique_vector_results = []
            for res in sorted(all_vector_results, key=lambda x: x['score'], reverse=True):
                if res['index'] not in seen_indices:
                    seen_indices.add(res['index'])
                    unique_vector_results.append(res)
            
            top_vector_results = unique_vector_results[:top_k * 2]

            # --- DEBUG: –ß–¢–û –ù–ê–®–ï–õ –í–ï–ö–¢–û–†? ---
            if top_vector_results:
                logger.info(f"   üëÄ –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö (–¢–æ–ø-3):")
                for i, res in enumerate(top_vector_results[:3]):
                    preview = res['text'][:100].replace('\n', ' ')
                    logger.info(f"      {i+1}. [{res['score']:.4f}] {preview}...")
            else:
                logger.info("   üëÄ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–µ–ª.")
            # --------------------------------

            # 4. Keyword Search (BM25)
            keyword_results = []
            if language in self.bm25_indices:
                keyword_results = self._search_by_keyword(query, language, top_k * 2)

            # 5. Simple Exact Phrase Search (NEW)
            simple_match_results = self._search_by_simple_match(query, language, top_k * 2)
            if simple_match_results:
                logger.info(f"   üìù –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(simple_match_results)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

            # 6. Hybrid Fusion (RRF - Reciprocal Rank Fusion)
            k_rrf = 60
            combined_scores = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ –µ—Å—Ç—å)
            for res in exact_results:
                idx = res['index']
                combined_scores[idx] = {'data': res, 'rrf_score': 100.0}

            # Process Vector Results
            for rank, res in enumerate(top_vector_results):
                idx = res['index']
                if idx not in combined_scores:
                    combined_scores[idx] = {'data': res, 'rrf_score': 0.0}
                if combined_scores[idx]['rrf_score'] < 50.0:
                    combined_scores[idx]['rrf_score'] += 1.0 / (k_rrf + rank + 1)
                    combined_scores[idx]['data']['vector_rank'] = rank + 1
                
            # Process BM25 Results
            for rank, res in enumerate(keyword_results):
                idx = res['index']
                if idx not in combined_scores:
                    combined_scores[idx] = {'data': res, 'rrf_score': 0.0}
                if combined_scores[idx]['rrf_score'] < 50.0:
                    # BM25 –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
                    combined_scores[idx]['rrf_score'] += 1.0 / (k_rrf + rank + 1)
                    combined_scores[idx]['data']['keyword_rank'] = rank + 1

            # Process Simple Match Results (NEW)
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å –≤—ã—Å–æ–∫–∏–π –≤–µ—Å
            for rank, res in enumerate(simple_match_results):
                idx = res['index']
                if idx not in combined_scores:
                    combined_scores[idx] = {'data': res, 'rrf_score': 0.0}
                if combined_scores[idx]['rrf_score'] < 50.0:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Å. –ï—Å–ª–∏ —Å–ª–æ–≤–æ —Ä–µ–¥–∫–æ–µ, —Ä–∞–Ω–≥ –±—É–¥–µ—Ç –≤—ã—Å–æ–∫–∏–º.
                    combined_scores[idx]['rrf_score'] += 1.0 / (k_rrf + rank + 1)
                    combined_scores[idx]['data']['simple_match_rank'] = rank + 1

            # Sort by RRF score
            hybrid_results = sorted(combined_scores.values(), key=lambda x: x['rrf_score'], reverse=True)
            
            # Extract top_k
            final_candidates = []
            for item in hybrid_results[:top_k]:
                res = item['data']
                res['score'] = item['rrf_score']
                final_candidates.append(res)
            
            logger.info(f"   ü§ù –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(final_candidates)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

            # 7. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (Re-ranking)
            if use_reranking and self.reranker.model:
                try:
                    logger.info("‚è≥ Starting Re-ranking process...")
                    docs_to_rerank = []
                    indices_to_rerank = []
                    final_results = []
                    
                    for i, res in enumerate(final_candidates):
                        if res['score'] > 50.0:
                            res['final_score'] = 1.0
                            final_results.append(res)
                        else:
                            docs_to_rerank.append(res['text'])
                            indices_to_rerank.append(i)
                    
                    if docs_to_rerank:
                        logger.info(f"   Reranking {len(docs_to_rerank)} documents...")
                        reranked_tuples = self.reranker.rerank(query, docs_to_rerank, len(docs_to_rerank))
                        
                        for original_idx_in_subset, score, text in reranked_tuples:
                            original_idx = indices_to_rerank[original_idx_in_subset]
                            original_result = final_candidates[original_idx]
                            original_result['final_score'] = float(score)
                            final_results.append(original_result)
                    else:
                        # If nothing to rerank (all exact matches), just copy
                        final_results.extend([res for res in final_candidates if 'final_score' not in res])
                    
                    logger.info("‚úÖ Re-ranking finished successfully.")

                except Exception as e:
                    logger.error(f"‚ùå Re-ranking failed (using standard results): {e}")
                    final_results = final_candidates
            else:
                if use_reranking:
                    logger.info("‚è© Skipping Re-ranking (model not loaded or disabled)")
                final_results = final_candidates

            return {
                'success': True,
                'results': final_results,
                'query_variants': query_variants,
                'count': len(final_results)
            }
        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}", exc_info=True)
            return {'success': False, 'error': str(e), 'query': query}

    def keyword_search(self, query: str, language: str = 'en', case_sensitive: bool = False) -> Dict[str, Any]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (standalone –º–µ—Ç–æ–¥).
        –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç, –Ω–æ –±–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ RRF pipeline.
        """
        logger.info(f"üîç Standalone Keyword search: '{query}'")
        
        if language not in self.languages:
            return {'success': False, 'error': f'Language {language} not supported'}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥, –µ—Å–ª–∏ —Ä–µ–≥–∏—Å—Ç—Ä –Ω–µ –≤–∞–∂–µ–Ω
        if not case_sensitive:
            results = self._search_by_simple_match(query, language, top_k=100)
            return {
                'success': True,
                'results': results,
                'query': query,
                'total_results': len(results),
                'language': language
            }
        
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω case_sensitive, –∏–¥–µ–º —Å—Ç–∞—Ä—ã–º –ø—É—Ç–µ–º
        metadata = self.metadata[language]
        results = []
        for item in metadata:
            text = self._get_text_from_meta(item, language)
            if query in text:
                results.append({
                    'text': text,
                    'book': item.get('book'),
                    'chapter': item.get('chapter'),
                    'score': 1.0
                })
        
        return {
            'success': True,
            'results': results,
            'query': query,
            'total_results': len(results),
            'language': language
        }