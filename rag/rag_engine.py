"""
üß† RAG ENGINE - –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å Re-ranking –¥–ª—è Shukabase

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Google Gemini API
2. FAISS –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
3. Re-ranking —Å –ø–æ–º–æ—â—å—é Jina Reranker
4. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Any
import logging
import os
import time

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
try:
    import faiss
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    import google.generativeai as genai
    from dotenv import load_dotenv
except ImportError as e:
    raise ImportError(
        f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: {e}. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã: pip install faiss-cpu transformers torch google-generativeai python-dotenv"
    )

logger = logging.getLogger(__name__)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (QueryExpander, RerankerModel) –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---

class QueryExpander:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    SYNONYMS_RU = {
        "–ª—é–±–æ–≤—å": ["–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å", "–±—Ö–∞–∫—Ç–∏", "–¥—Ä—É–∂–±–∞", "–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å"],
        "–±–æ–≥": ["–∫—Ä–∏—à–Ω–∞", "–≤–µ—Ä—Ö–æ–≤–Ω—ã–π", "–∞–±—Å–æ–ª—é—Ç", "–±–æ–∂–µ—Å—Ç–≤–æ"],
        "–¥—É—à–∞": ["–∞—Ç–º–∞", "–¥—É—Ö", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Å—É—â–Ω–æ—Å—Ç—å"],
        "–∑–Ω–∞–Ω–∏–µ": ["–¥–∂–Ω—è–Ω–∞", "–º—É–¥—Ä–æ—Å—Ç—å", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ"],
        "–π–æ–≥–∞": ["–ø—Ä–∞–∫—Ç–∏–∫–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "–ø—É—Ç—å"],
        "–∫–∞—Ä–º–∞": ["–¥–µ–π—Å—Ç–≤–∏–µ", "–¥–µ—è–Ω–∏–µ", "—Å–ª–µ–¥—Å—Ç–≤–∏–µ", "—Å—É–¥—å–±–∞"],
        "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ": ["–º–æ–∫—à–∞", "—Å–ø–∞—Å–µ–Ω–∏–µ", "—Å–≤–æ–±–æ–¥–∞", "–≤—ã—Ö–æ–¥"],
        "–º–∏—Ä": ["–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π", "–≤—Å–µ–ª–µ–Ω–Ω–∞—è", "–≤—Ä–µ–º–µ–Ω–Ω—ã–π", "–ø—Ä–µ—Ö–æ–¥—è—â–∏–π"],
    }
    
    SYNONYMS_EN = {
        "love": ["devotion", "bhakti", "affection", "attachment"],
        "god": ["krishna", "supreme", "absolute", "deity"],
        "soul": ["atma", "spirit", "consciousness", "essence"],
        "knowledge": ["jnana", "wisdom", "understanding", "realization"],
        "yoga": ["practice", "meditation", "discipline", "path"],
        "karma": ["action", "deed", "consequence", "fate"],
        "liberation": ["moksha", "salvation", "freedom", "release"],
        "world": ["material", "universe", "temporary", "transient"],
    }
    
    @staticmethod
    def expand_query_ru(query: str) -> List[str]:
        expanded = [query]
        query_lower = query.lower()
        for term, synonyms in QueryExpander.SYNONYMS_RU.items():
            if term in query_lower:
                for synonym in synonyms[:2]:
                    expanded.append(query.lower().replace(term, synonym))
        return list(set(expanded))[:3]
    
    @staticmethod
    def expand_query_en(query: str) -> List[str]:
        expanded = [query]
        query_lower = query.lower()
        for term, synonyms in QueryExpander.SYNONYMS_EN.items():
            if term in query_lower:
                for synonym in synonyms[:2]:
                    expanded.append(query.lower().replace(term, synonym))
        return list(set(expanded))[:3]


class RerankerModel:
    """–ú–æ–¥–µ–ª—å re-ranking –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, model_name: str = "jinaai/jina-reranker-v2-base-multilingual"):
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å re-ranking: {model_name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name, trust_remote_code=True, torch_dtype=torch.float32
            )
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å re-ranking –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (device: {self.device})")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ re-ranking: {e}")
            self.model = None
    
    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float, str]]:
        if not self.model or not documents:
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]
        try:
            with torch.no_grad():
                inputs = self.tokenizer(
                    [[query, doc] for doc in documents],
                    padding=True, truncation=True, return_tensors="pt", max_length=512
                ).to(self.device)
                scores = self.model(**inputs, return_dict=True).logits.squeeze(-1).cpu().numpy()
            
            ranked = sorted([(i, score, documents[i]) for i, score in enumerate(scores)], key=lambda x: x[1], reverse=True)
            return ranked[:top_k]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ re-ranking: {e}")
            return [(i, 1.0, doc) for i, doc in enumerate(documents)][:top_k]


# --- –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π RAGEngine ---

class RAGEngine:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å RAG —Å–∏—Å—Ç–µ–º—ã —Å Google Gemini API"""
    
    def __init__(
        self,
        reranker_model: str = "jinaai/jina-reranker-v2-base-multilingual",
        languages: List[str] = ['ru', 'en'],
        base_dir: str = "rag"
    ):
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é RAG Engine...")
        
        self._configure_gemini_api()
        
        self.base_dir = Path(base_dir)
        self.embedding_model_name = "models/text-embedding-004"
        self.languages = languages
        
        self.reranker = RerankerModel(reranker_model)
        
        self.indices: Dict[str, faiss.Index] = {}
        self.metadata: Dict[str, Any] = {}
        self.chunked_data: Dict[str, Dict] = {}
        
        for lang in languages:
            self._load_language_data(lang)
        
        logger.info("‚úÖ RAG Engine –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _configure_gemini_api(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–ª—é—á API –¥–ª—è Gemini."""
        load_dotenv()
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        try:
            genai.configure(api_key=api_key)
            logger.info("‚úÖ –ö–ª—é—á Gemini API —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω.")
        except Exception as e:
            raise RuntimeError(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini API: {e}")

    def _load_language_data(self, language: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —á–∞–Ω–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."""
        index_file = self.base_dir / f"faiss_index_{language}.bin"
        metadata_file = self.base_dir / f"faiss_metadata_{language}.json"
        chunks_file = self.base_dir / f"chunked_scriptures_{language}.json"

        if not index_file.exists():
            logger.warning(f"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å FAISS –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_file}")
            return
            
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è —è–∑—ã–∫–∞ '{language}'...")
        self.indices[language] = faiss.read_index(str(index_file))
        logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.indices[language].ntotal:,} –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ {index_file}")

        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                raw_metadata = json.load(f)
            
            # Flatten metadata to match FAISS indices
            flat_metadata = []
            structure = raw_metadata.get('structure', {})
            
            # Collect all chapters
            all_chapters = []
            for book_key, book_data in structure.items():
                for chapter_key, chapter_data in book_data.items():
                    if 'embedding_key' in chapter_data:
                        all_chapters.append({
                            'book': book_key,
                            'chapter': chapter_key,
                            'data': chapter_data
                        })
            
            # Sort by embedding_key index (e.g., embeddings_0, embeddings_1)
            def get_embedding_index(item):
                key = item['data']['embedding_key']
                try:
                    return int(key.split('_')[1])
                except (IndexError, ValueError):
                    return 999999
            
            all_chapters.sort(key=get_embedding_index)
            
            # Create flat list
            for item in all_chapters:
                book = item['book']
                chapter = item['chapter']
                data = item['data']
                num_chunks = data.get('num_chunks', 0)
                text_previews = data.get('text_previews', [])
                
                for i in range(num_chunks):
                    preview = text_previews[i] if i < len(text_previews) else ""
                    flat_metadata.append({
                        'book': book,
                        'chapter': chapter,
                        'chunk_idx': i,
                        'text_preview': preview
                    })
            
            self.metadata[language] = flat_metadata
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ({len(flat_metadata)} –∑–∞–ø–∏—Å–µ–π)")
        else:
            logger.warning(f"  - –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_file}")

        if chunks_file.exists():
            with open(chunks_file, 'r', encoding='utf-8') as f:
                self.chunked_data[language] = json.load(f)
            logger.info(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω—ã —á–∞–Ω–∫–∏ –∏–∑ {chunks_file}")
        else:
             logger.warning(f"  - –§–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {chunks_file}")

    def _get_embedding(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Gemini API."""
        try:
            # RETRIEVAL_QUERY –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if len(texts) == 1:
                # –î–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                result = genai.embed_content(
                    model=self.embedding_model_name,
                    content=texts[0],
                    task_type="RETRIEVAL_QUERY"
                )
                embedding = result['embedding']
                return np.array([embedding], dtype='float32')
            else:
                # –î–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
                all_embeddings = []
                for text in texts:
                    result = genai.embed_content(
                        model=self.embedding_model_name,
                        content=text,
                        task_type="RETRIEVAL_QUERY"
                    )
                    all_embeddings.append(result['embedding'])
                return np.array(all_embeddings, dtype='float32')
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ—Ç Gemini API: {e}", exc_info=True)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–∞–¥–µ–Ω–∏—è
            dim = 768 # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è text-embedding-004
            return np.zeros((len(texts), dim), dtype='float32')


    def _search_by_vector(self, query_embedding: np.ndarray, language: str, top_k: int, vector_distance_threshold: float = None) -> List[Dict[str, Any]]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ FAISS."""
        index = self.indices.get(language)
        if not index: return []

        try:
            query_norm = query_embedding.copy().reshape(1, -1)
            faiss.normalize_L2(query_norm)
            distances, indices_found = index.search(query_norm, top_k * 2) # –ò—â–µ–º —Å –∑–∞–ø–∞—Å–æ–º
            
            results = []
            metadata_list = self.metadata.get(language, [])
            chunks_map = self.chunked_data.get(language, {})
            
            seen_ids = set()
            
            for i, (dist, idx) in enumerate(zip(distances[0], indices_found[0])):
                if idx < 0: continue

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                if vector_distance_threshold is not None and dist > vector_distance_threshold:
                    continue


                meta = metadata_list[idx] if isinstance(metadata_list, list) and idx < len(metadata_list) else {}
                book, chapter = meta.get('book'), meta.get('chapter')
                chunk_idx = meta.get('chunk_idx')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                unique_id = f"{book}_{chapter}_{chunk_idx}"
                if unique_id in seen_ids:
                    continue
                seen_ids.add(unique_id)
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ chunked_data
                text = ""
                if book and chapter and book in chunks_map and chapter in chunks_map[book]:
                    chapter_chunks = chunks_map[book][chapter]
                    if isinstance(chapter_chunks, list) and isinstance(chunk_idx, int):
                        if 0 <= chunk_idx < len(chapter_chunks):
                            text = chapter_chunks[chunk_idx]
                
                if not text:
                    text = meta.get('text_preview', '') + '...'

                results.append({
                    'index': int(idx),
                    'distance': float(dist),
                    'score': float(1.0 / (1.0 + dist)),
                    'text': text,
                    'book': book, 
                    'chapter': chapter, 
                    'verse': None, 
                    'chunk_idx': chunk_idx
                })
                
                if len(results) >= top_k:
                    break

            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –≤–µ–∫—Ç–æ—Ä—É ({language}): {e}", exc_info=True)
            return []

    def search(
        self, 
        query: str, 
        language: str = 'ru', 
        top_k: int = 5, 
        use_reranking: bool = True,
        expand_query: bool = True,
        vector_distance_threshold: float = None
    ) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞."""
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' ({language}, top_k={top_k})")
        if language not in self.indices:
            return {'success': False, 'error': f'–ò–Ω–¥–µ–∫—Å –¥–ª—è —è–∑—ã–∫–∞ {language} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.'}

        try:
            # 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            query_variants = [query]
            if expand_query:
                expander_method = getattr(QueryExpander, f'expand_query_{language}', None)
                if expander_method:
                    query_variants = expander_method(query)
            
            logger.info(f"   üìã –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞: {query_variants}")
            
            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
            variant_embeddings = self._get_embedding(query_variants)
            logger.info(f"   üî¢ –ü–æ–ª—É—á–µ–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {variant_embeddings.shape}")
            
            # 3. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
            all_results = []
            for idx, emb in enumerate(variant_embeddings):
                vector_results = self._search_by_vector(emb, language, top_k * 2, vector_distance_threshold)
                logger.debug(f"   üîé –í–∞—Ä–∏–∞–Ω—Ç '{query_variants[idx]}': –Ω–∞–π–¥–µ–Ω–æ {len(vector_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                all_results.extend(vector_results)
            
            # 4. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö
            seen_indices = set()
            unique_results = []
            for res in sorted(all_results, key=lambda x: x['score'], reverse=True):
                if res['index'] not in seen_indices:
                    seen_indices.add(res['index'])
                    unique_results.append(res)
            
            top_results = unique_results[:top_k]

            # 5. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            if use_reranking and self.reranker.model:
                docs_to_rerank = [r['text'] for r in top_results]
                reranked_tuples = self.reranker.rerank(query, docs_to_rerank, top_k)
                
                # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ re-ranker'–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                final_results = []
                for original_idx, score, text in reranked_tuples:
                    original_result = top_results[original_idx]
                    original_result['final_score'] = float(score)
                    final_results.append(original_result)
            else:
                final_results = top_results

            return {
                'success': True,
                'results': final_results,
                'query_variants': query_variants,
                'count': len(final_results)
            }
        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}", exc_info=True)
            return {'success': False, 'error': str(e), 'query': query}
