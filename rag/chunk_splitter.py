#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚úÇÔ∏è  –†–ê–ó–ë–ò–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ù–ê –ß–ê–ù–ö–ò –î–õ–Ø RAG

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
—Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ó–ê–ü–£–°–ö:
    python rag/chunk_splitter.py
"""

import json
from pathlib import Path
from typing import List, Dict
import time


class ChunkSplitter:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    
    def __init__(self, chunk_size=2048, overlap=256):
        """
        Args:
            chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (—É–≤–µ–ª–∏—á–µ–Ω —Å 512 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
            overlap: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 64)
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def split_text(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        if len(text) <= self.chunk_size:
            return [text] if text.strip() else []
        
        chunks = []
        start = 0
        iterations = 0
        max_iterations = (len(text) // max(self.chunk_size - self.overlap, 1)) + 100
        
        while start < len(text):
            iterations += 1
            
            # –ó–ê–©–ò–¢–ê: –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π, –∑–Ω–∞—á–∏—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
            if iterations > max_iterations:
                remaining = text[start:].strip()
                if remaining:
                    chunks.append(remaining)
                break
            
            # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω–µ—á–Ω—É—é –ø–æ–∑–∏—Ü–∏—é —á–∞–Ω–∫–∞
            end = min(start + self.chunk_size, len(text))
            
            # –ï—Å–ª–∏ –Ω–µ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞, –∏—â–µ–º —Ö–æ—Ä–æ—à—É—é —Ç–æ—á–∫—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            if end < len(text):
                search_start = max(start, end - 100)
                search_area = text[search_start:end]
                
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É/–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ/–≤–æ–ø—Ä–æ—Å (–ë–ï–ó regex –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
                for i in range(len(search_area) - 1, -1, -1):
                    if search_area[i] in '.!?':
                        end = search_start + i + 1
                        break
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–∫–∏, –∏—â–µ–º –ø—Ä–æ–±–µ–ª
                    last_space = text.rfind(' ', start, end)
                    if last_space > start:
                        end = last_space
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã –¥–≤–∏–≥–∞–µ–º—Å—è –≤–ø–µ—Ä–µ–¥
            step = self.chunk_size - self.overlap
            if step <= 0:
                step = max(1, self.chunk_size // 2)
            
            old_start = start
            start = end - self.overlap
            
            # –ï—Å–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–ª–∏—à–∫–æ–º –º–∞–ª, –¥–µ–ª–∞–µ–º –±–æ–ª—å—à–∏–π –ø—Ä—ã–∂–æ–∫
            if start <= old_start + step // 10:
                start = end
        
        return chunks
    
    def chunk_parsed_scripture(self, parsed_data: Dict, language: str = 'ru'):
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –ø–∏—Å–∞–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            parsed_data: —Å–ª–æ–≤–∞—Ä—å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –ø–∏—Å–∞–Ω–∏–π
            language: —è–∑—ã–∫ ('ru' –∏–ª–∏ 'en')
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —á–∞–Ω–∫–∞–º–∏
        """
        chunked_data = {}
        total_chunks = 0
        
        for book_name in sorted(parsed_data.keys()):
            print(f"  üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ {book_name}...")
            
            chunked_data[book_name] = {}
            book_chunks = 0
            file_count = 0
            
            for file_path, text in parsed_data[book_name].items():
                file_count += 1
                chunks = self.split_text(text)
                
                if chunks:
                    chunked_data[book_name][file_path] = chunks
                    book_chunks += len(chunks)
                    total_chunks += len(chunks)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 —Ñ–∞–π–ª–æ–≤
                if file_count % 100 == 0:
                    print(f"    ‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {file_count} —Ñ–∞–π–ª–æ–≤... ({total_chunks} —á–∞–Ω–∫–æ–≤)")
            
            print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ {book_chunks} —á–∞–Ω–∫–æ–≤ –∏–∑ {file_count} —Ñ–∞–π–ª–æ–≤")
        
        return chunked_data, total_chunks
    
    def process_language(self, language: str = 'ru') -> tuple:
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞
        
        Args:
            language: 'ru' –∏–ª–∏ 'en'
            
        Returns:
            (chunked_data, stats_dict)
        """
        parsed_file = f"rag/parsed_scriptures_{language}.json"
        output_file = f"rag/chunked_scriptures_{language}.json"
        
        if not Path(parsed_file).exists():
            print(f"‚ö†Ô∏è  –§–∞–π–ª {parsed_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {language}.")
            return None, None
        if Path(output_file).exists():
            print(f"‚è© {output_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {language}.")
            file_size = Path(output_file).stat().st_size / (1024*1024)
            stats = {
                'language': language,
                'total_books': None,
                'total_chunks': None,
                'chunk_size': self.chunk_size,
                'overlap': self.overlap,
                'output_file': output_file,
                'file_size_mb': file_size,
                'elapsed_seconds': 0
            }
            return None, stats
        print(f"\nüîç –ó–∞–≥—Ä—É–∂–∞—é {parsed_file}...")
        start_time = time.time()
        with open(parsed_file, 'r', encoding='utf-8') as f:
            parsed_data = json.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(parsed_data)} –∫–Ω–∏–≥")
        print(f"\n‚úÇÔ∏è  –†–∞–∑–±–∏–≤–∞—é —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ (chunk_size={self.chunk_size}, overlap={self.overlap})...")
        chunked_data, total_chunks = self.chunk_parsed_scripture(parsed_data, language)
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ {output_file}...")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunked_data, f, ensure_ascii=False, indent=2)
        file_size = Path(output_file).stat().st_size / (1024*1024)
        elapsed = time.time() - start_time
        print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω! –†–∞–∑–º–µ—Ä: {file_size:.2f} –ú–ë")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {elapsed:.1f} —Å–µ–∫ ({elapsed/60:.1f} –º–∏–Ω)")
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = {
            'language': language,
            'total_books': len(chunked_data),
            'total_chunks': total_chunks,
            'chunk_size': self.chunk_size,
            'overlap': self.overlap,
            'output_file': output_file,
            'file_size_mb': file_size,
            'elapsed_seconds': elapsed
        }
        return chunked_data, stats


def process_all_languages():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±–∞ —è–∑—ã–∫–∞"""
    
    print("="*70)
    print("‚úÇÔ∏è  –†–ê–ó–ë–ò–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ù–ê –ß–ê–ù–ö–ò")
    print("="*70)
    
    import sys
    splitter = ChunkSplitter(chunk_size=2048, overlap=256)
    all_stats = {}
    langs = []
    # CLI: python chunk_splitter.py [ru|en|all]
    if len(sys.argv) > 1:
        arg = sys.argv[1].lower()
        if arg in ('ru', 'en'):
            langs = [arg]
        else:
            langs = ['ru', 'en']
    else:
        langs = ['ru', 'en']
    for lang in langs:
        print(f"\nüìç –≠–¢–ê–ü: {lang.upper()} –ü–ò–°–ê–ù–ò–Ø")
        print("-" * 70)
        _, stats = splitter.process_language(lang)
        all_stats[lang] = stats
    print("\n" + "="*70)
    print("‚úÖ –†–ê–ó–ë–ò–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*70)
    total_time = sum(stats['elapsed_seconds'] for stats in all_stats.values() if stats and 'elapsed_seconds' in stats)
    for lang, stats in all_stats.items():
        print(f"\nüìä {lang.upper()}:")
        if stats:
            print(f"   üìö –ö–Ω–∏–≥: {stats['total_books']}")
            print(f"   üìÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats['total_chunks']}")
            print(f"   üíæ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {stats['file_size_mb']:.2f} –ú–ë")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {stats['elapsed_seconds']:.1f} —Å–µ–∫")
            print(f"   üìù –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: chunk_size={stats['chunk_size']}, overlap={stats['overlap']}")
        else:
            print("   ‚ö†Ô∏è  –ù–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ.")
    print(f"\n‚è±Ô∏è  –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_time:.1f} —Å–µ–∫ ({total_time/60:.2f} –º–∏–Ω)")
    print("\nüëâ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è RAG")
    return all_stats


if __name__ == "__main__":
    process_all_languages()
