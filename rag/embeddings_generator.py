#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üß† –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –î–õ–Ø RAG

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏) —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞
–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ –ø–æ–∏—Å–∫–∞ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É.
–û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Google Gemini API.

–ó–ê–ü–£–°–ö:
    python rag/embeddings_generator.py
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List
import time
import os
import google.generativeai as genai
from dotenv import load_dotenv

class EmbeddingsGenerator:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Google Gemini API"""
    
    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å text-embedding-004.
        """
        self.model_name = "models/text-embedding-004"
        self.embedding_dim = 768  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è text-embedding-004
        print(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –º–æ–¥–µ–ª—å—é: {self.model_name}")
        print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {self.embedding_dim}")
    
    def generate_embeddings(self, chunks_data: Dict[str, Dict[str, List[str]]], 
                          language: str = 'ru', batch_size: int = 100) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ Google Gemini API
        
        Args:
            chunks_data: —Å–ª–æ–≤–∞—Ä—å —Å —á–∞–Ω–∫–∞–º–∏
            language: —è–∑—ã–∫ ('ru' –∏–ª–∏ 'en')
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (max 100 –¥–ª—è Gemini API)
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if batch_size > 100:
            print(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ ({batch_size}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç API (100). –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é 100.")
            batch_size = 100

        embeddings_data = {
            'model': self.model_name,
            'embedding_dim': self.embedding_dim,
            'language': language,
            'books': {}
        }
        
        total_chunks = 0
        total_embeddings = 0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        all_chunks_with_info = []
        
        for book_name in sorted(chunks_data.keys()):
            embeddings_data['books'][book_name] = {}
            
            for file_path in sorted(chunks_data[book_name].keys()):
                embeddings_data['books'][book_name][file_path] = []
                
                for chunk_idx, chunk_text in enumerate(chunks_data[book_name][file_path]):
                    all_chunks_with_info.append({
                        'text': chunk_text,
                        'book': book_name,
                        'file': file_path,
                        'chunk_idx': chunk_idx
                    })
                    total_chunks += 1
        
        print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_chunks:,}")
        print(f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (batch_size={batch_size}). –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è...\n")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        start_time = time.time()
        
        for batch_start in range(0, len(all_chunks_with_info), batch_size):
            batch_end = min(batch_start + batch_size, len(all_chunks_with_info))
            batch_info = all_chunks_with_info[batch_start:batch_end]
            
            texts = [item['text'] for item in batch_info]
            
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ API
                result = genai.embed_content(
                    model=self.model_name,
                    content=texts,
                    task_type="RETRIEVAL_DOCUMENT" # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                )
                batch_embeddings = result['embedding']
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
                for i, item in enumerate(batch_info):
                    embedding = batch_embeddings[i]
                    embeddings_data['books'][item['book']][item['file']].append({
                        'chunk_idx': item['chunk_idx'],
                        'text_preview': item['text'][:100],
                        'embedding': embedding
                    })
                    total_embeddings += 1

                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_pct = (batch_end / len(all_chunks_with_info)) * 100
                elapsed = time.time() - start_time
                rate = total_embeddings / elapsed if elapsed > 0 else 0
                eta = (len(all_chunks_with_info) - total_embeddings) / rate if rate > 0 else 0
                
                print(f"  ‚è≥ {progress_pct:5.1f}% | {total_embeddings:7,} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ | {rate:5.1f} —à—Ç/—Å–µ–∫ | ETA: {eta:6.0f}—Å–µ–∫")

                # –ü–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã API (–Ω–∞–ø—Ä–∏–º–µ—Ä, 60 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
                time.sleep(1)

            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {batch_start}-{batch_end}: {e}")
                print("   –ü—Ä–æ–ø—É—Å–∫–∞—é —ç—Ç–æ—Ç –±–∞—Ç—á. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ API –∫–ª—é—á.")
                continue

        elapsed = time.time() - start_time
        print(f"\n‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {elapsed:.1f} —Å–µ–∫ ({elapsed/60:.1f} –º–∏–Ω)")
        
        return embeddings_data
    
    def save_embeddings(self, embeddings_data: Dict, language: str = 'ru'):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ñ–∞–π–ª (–≤ —Å–∂–∞—Ç–æ–º –≤–∏–¥–µ —Å NumPy)
        """
        # ... (—ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        output_file = f"rag/embeddings_{language}.npz"
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ {output_file}...")
        
        embeddings_arrays = {}
        metadata = {
            'model': embeddings_data['model'],
            'embedding_dim': embeddings_data['embedding_dim'],
            'language': embeddings_data['language'],
            'structure': {}
        }
        
        idx = 0
        for book_name in sorted(embeddings_data['books'].keys()):
            metadata['structure'][book_name] = {}
            
            for file_path in sorted(embeddings_data['books'][book_name].keys()):
                chunk_list = embeddings_data['books'][book_name][file_path]
                
                if chunk_list:
                    embeddings_matrix = np.array([item['embedding'] for item in chunk_list])
                    key = f"embeddings_{idx}"
                    embeddings_arrays[key] = embeddings_matrix
                    
                    metadata['structure'][book_name][file_path] = {
                        'embedding_key': key,
                        'num_chunks': len(chunk_list),
                        'text_previews': [item['text_preview'] for item in chunk_list]
                    }
                    idx += 1
        
        np.savez_compressed(output_file, **embeddings_arrays)
        
        metadata_file = f"rag/embeddings_metadata_{language}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        npz_size = Path(output_file).stat().st_size / (1024*1024)
        json_size = Path(metadata_file).stat().st_size / (1024*1024)
        
        print(f"‚úÖ NPZ —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {npz_size:.2f} –ú–ë")
        print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_size:.2f} –ú–ë")
        
        return output_file, metadata_file

    def process_language(self, language: str = 'ru'):
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞
        """
        chunked_file = f"rag/chunked_scriptures_{language}.json"
        
        print(f"\nüîç –ó–∞–≥—Ä—É–∂–∞—é —á–∞–Ω–∫–∏ –∏–∑ {chunked_file}...")
        with open(chunked_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks_data)} –∫–Ω–∏–≥")
        
        embeddings_data = self.generate_embeddings(chunks_data, language=language, batch_size=100)
        
        if sum(len(file_data) for book_data in embeddings_data['books'].values() for file_data in book_data.values()) == 0:
            print("‚ùå –ù–µ –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω.")
            return None

        npz_file, json_file = self.save_embeddings(embeddings_data, language=language)
        
        stats = {
            'language': language,
            'total_books': len(embeddings_data['books']),
            'embedding_model': embeddings_data['model'],
            'embedding_dim': embeddings_data['embedding_dim'],
            'npz_file': npz_file,
            'metadata_file': json_file
        }
        
        return stats


def process_all_languages():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö —è–∑—ã–∫–æ–≤"""
    
    print("="*70)
    print("üß† –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –î–õ–Ø RAG (GOOGLE GEMINI API)")
    print("="*70)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º API –∫–ª—é—á –∏–∑ .env —Ñ–∞–π–ª–∞
    load_dotenv()
    if 'GEMINI_API_KEY' not in os.environ:
        print("‚ùå –û–®–ò–ë–ö–ê: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        print("   –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –≤ –Ω–µ–≥–æ —Å—Ç—Ä–æ–∫—É:")
        print("   GEMINI_API_KEY='–í–∞—à_–∫–ª—é—á'")
        return
    
    try:
        genai.configure(api_key=os.environ['GEMINI_API_KEY'])
        print("‚úÖ –ö–ª—é—á Gemini API —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini API: {e}")
        return

    generator = EmbeddingsGenerator()
    
    all_stats = {}
    
    # –†—É—Å—Å–∫–∏–µ –ø–∏—Å–∞–Ω–∏—è
    print("\nüìç –≠–¢–ê–ü 1: –≠–ú–ë–ï–î–î–ò–ù–ì–ò –î–õ–Ø –†–£–°–°–ö–ò–• –ü–ò–°–ê–ù–ò–ô")
    print("-" * 70)
    stats_ru = generator.process_language('ru')
    if stats_ru:
        all_stats['ru'] = stats_ru
    
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –ø–∏—Å–∞–Ω–∏—è
    print("\nüìç –≠–¢–ê–ü 2: –≠–ú–ë–ï–î–î–ò–ù–ì–ò –î–õ–Ø –ê–ù–ì–õ–ò–ô–°–ö–ò–• –ü–ò–°–ê–ù–ò–ô")
    print("-" * 70)
    stats_en = generator.process_language('en')
    if stats_en:
        all_stats['en'] = stats_en
    
    print("\n" + "="*70)
    if not all_stats:
        print("‚ùå –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ó–ê–í–ï–†–®–ò–õ–ê–°–¨ –° –û–®–ò–ë–ö–ê–ú–ò.")
    else:
        print("‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("="*70)
    
    for lang, stats in all_stats.items():
        print(f"\nüìä {lang.upper()}:")
        print(f"   üìö –ö–Ω–∏–≥: {stats['total_books']}")
        print(f"   üß† –ú–æ–¥–µ–ª—å: {stats['embedding_model']}")
        print(f"   üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {stats['embedding_dim']}")
        print(f"   üíæ NPZ —Ñ–∞–π–ª: {stats['npz_file']}")
        print(f"   üìù –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {stats['metadata_file']}")
    
    if all_stats:
        print("\nüëâ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ (FAISS)")
    
    return all_stats


if __name__ == "__main__":
    process_all_languages()
